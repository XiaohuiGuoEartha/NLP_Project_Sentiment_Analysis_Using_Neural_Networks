{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Homework 3\n",
    "## Sentiment analysis using Neural Networks\n",
    "\n",
    "Total: 50 Points\n",
    "\n",
    "\n",
    "In this homework we will perform sentiment analysis using a few simple Neural Network based architectures.\n",
    "For this problem we use the IMDB Large Movie Review Dataset. The dataset contains 25,000 highly polar movie reviews for both train and test dataset, each with 12,500 positive (greater than equal to 7/10 rating) and 12,500 negative reviews(less than equal to 4/10 rating). \n",
    "\n",
    "Use \"https://keras.io/\" for keras documentation. Please use Python 3. GPU is not required but it will help improve the training speed for each problem.\n",
    "\n",
    "Please save the notebook with your cell outputs. You will not be graded if your outputs are not present below the homework cell. Also note your outputs will be unique since you will be using your the last numbers of your uni as your random seed (In the third cell). Make sure you submit this iPython file, with the saved outputs. The submission format must be 'hw3/hw3.ipynb'. You will not submit any other files. If you do save your model weights, you will not submit them. You will however, make sure your model weights do get saved in the 'weights' folder and can be retrieved from there as well.\n",
    "\n",
    "Please fill your details below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Name: Xiaohui Guo\n",
    "\n",
    "Uni: xg2225\n",
    "\n",
    "Email: xg2225@columbia.edu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "import random\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Dropout, Reshape, Merge, BatchNormalization, TimeDistributed, Lambda, Activation, LSTM, Flatten, Convolution1D, GRU, MaxPooling1D\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
    "#from keras import initializers\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "from keras import optimizers\n",
    "import numpy as np\n",
    "import h5py\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#we retrieve train and test file names\n",
    "\n",
    "train_dir = \"./aclImdb/train/\"\n",
    "test_dir = \"./aclImdb/test/\"\n",
    "tr_review = [re_filename for re_filename in listdir(train_dir)]\n",
    "te_review = [re_filename for re_filename in listdir(test_dir)]\n",
    "\n",
    "#we initialize the train and test arrays\n",
    "\n",
    "tr_X = []\n",
    "tr_Y = []\n",
    "te_X = []\n",
    "te_Y = []\n",
    "\n",
    "#we arrange the reviews into the train and test arrays \n",
    "\n",
    "for review_file in tr_review:\n",
    "    f_review = open(train_dir+review_file, \"r\")\n",
    "    str_review = f_review.readline()\n",
    "    str_review = \" \".join(str_review.split(' '))\n",
    "    tr_X.append(str_review)\n",
    "    y_truth = int (review_file.split('.')[0].split('_')[1])\n",
    "    if y_truth>=7:\n",
    "        tr_Y.append(1)\n",
    "    else:\n",
    "        tr_Y.append(0)\n",
    "        \n",
    "for review_file in te_review:\n",
    "    f_review = open(test_dir+review_file, \"r\")\n",
    "    str_review = f_review.readline()\n",
    "    str_review = \" \".join(str_review.split(' '))\n",
    "    te_X.append(str_review)\n",
    "    y_truth = int (review_file.split('.')[0].split('_')[1])\n",
    "    if y_truth>=7:\n",
    "        te_Y.append(1)\n",
    "    else:\n",
    "        te_Y.append(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create the validation set from the train set\n",
    "\n",
    "use the last 4 numbers of your uni for the seed value seed to ensure all answers remain unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2517\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#replace 2 (SEED) with the last 4 numbers of your Uni\n",
    "#Uni: \n",
    "SEED = 2225\n",
    "seed_counter = 0\n",
    "while(1):\n",
    "\n",
    "    shuffle_combine = list(zip(tr_X, tr_Y))\n",
    "    random.seed(SEED+seed_counter)\n",
    "    seed_counter+=1\n",
    "    random.shuffle(shuffle_combine)\n",
    "\n",
    "    tr_X, tr_Y = zip(*shuffle_combine)\n",
    "\n",
    "    val_X = tr_X[:5000]\n",
    "    val_Y = tr_Y[:5000]\n",
    "\n",
    "    counter = 0\n",
    "    for label in val_Y:\n",
    "        counter+=label\n",
    "\n",
    "    print (counter)\n",
    "    print (seed_counter)\n",
    "    if(counter>2400 and counter <2600):\n",
    "        tr_X = tr_X[5000:]\n",
    "        tr_Y = tr_Y[5000:]\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Train review set : 20000\n",
      "Length of Train label set : 20000\n",
      "Length of Validation review set : 5000\n",
      "Length of Validation label set : 5000\n",
      "Length of Test review set : 25000\n",
      "Length of Test label set : 25000\n",
      "*****************************************\n",
      "Some sample Reviews Train sets and their labels\n",
      "Having to have someone hold your hand whenever walk up or down stairs? Having others taste your food before you eat it? Facing an over-bearing mother?\n",
      "1\n",
      "This is a hard-boiled Warner Brothers film starring a very young Barbara Stanwyck. A consummate master at portraying Machiavellian cool, a technique s\n",
      "1\n",
      "Its hard to make heads or tails of this film. Unless you're well oiled and in the mood to mock, don't view Santa Claus. It mixes Santa, Satan, Merlin,\n",
      "0\n",
      "The best of the seven Sam Fuller movies that I've seen (including Park Row, Run of the Arrow, Verboten!, Shock Corridor, The Naked Kiss, The Big Red O\n",
      "1\n",
      "This is a great show, and will make you cry, this group people really loved each other in real life and it shows time and time again. Email me and let\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"Length of Train review set : \" + str(len(tr_X)))\n",
    "print(\"Length of Train label set : \" + str(len(tr_Y)))\n",
    "print(\"Length of Validation review set : \" + str(len(val_X)))\n",
    "print(\"Length of Validation label set : \" + str(len(val_Y)))\n",
    "print(\"Length of Test review set : \" + str(len(te_X)))\n",
    "print(\"Length of Test label set : \" + str(len(te_Y)))\n",
    "print(\"*****************************************\")\n",
    "print(\"Some sample Reviews Train sets and their labels\")\n",
    "print(tr_X[0][:150])\n",
    "print(tr_Y[0])\n",
    "print(tr_X[1][:150])\n",
    "print(tr_Y[1])\n",
    "print(tr_X[2][:150])\n",
    "print(tr_Y[2])\n",
    "print(tr_X[3][:150])\n",
    "print(tr_Y[3])\n",
    "print(tr_X[4][:150])\n",
    "print(tr_Y[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we collect all the reviews from train validation and test set to generate \n",
    "texts = []\n",
    "texts += tr_X \n",
    "texts += te_X \n",
    "texts += val_X\n",
    "len(texts)\n",
    "\n",
    "\n",
    "\n",
    "#we clip the sentence length to first 250 words. \n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "\n",
    "#length of vocab, Tokenizer will only use vocab_len most common words\n",
    "vocab_len = 25000\n",
    "\n",
    "#we tokenize the texts and convert all the words to tokens\n",
    "tokenizer = Tokenizer(num_words=vocab_len)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "token_tr_X = tokenizer.texts_to_sequences(tr_X)\n",
    "token_te_X = tokenizer.texts_to_sequences(te_X)\n",
    "token_val_X = tokenizer.texts_to_sequences(val_X)\n",
    "\n",
    "#to ensure all reviews have the same length, we pad the smaller reviews with 0, \n",
    "#and cut the larger reviews to a max length \n",
    "#(we clip from the top, as the end of the reviews generally have a conclusion which provides better features)\n",
    "x_train = sequence.pad_sequences(token_tr_X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "x_test = sequence.pad_sequences(token_te_X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "x_val = sequence.pad_sequences(token_val_X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "#changes the labels to one-hot encoding\n",
    "y_train = np_utils.to_categorical(tr_Y)\n",
    "y_test = np_utils.to_categorical(te_Y)\n",
    "y_val = np_utils.to_categorical(val_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (20000, 250)\n",
      "X_test shape: (25000, 250)\n",
      "X_val shape: (5000, 250)\n",
      "y_train shape: (20000, 2)\n",
      "y_test shape: (25000, 2)\n",
      "y_val shape: (5000, 2)\n",
      "*****************************************\n",
      "Tokenized Reviews Train sets and their labels\n",
      "[ 770 8370    6  898    2  162   32 1598    8    1   19    7    7  259    1\n",
      "   17  124 2432    5 1861]\n",
      "[ 0.  1.]\n",
      "\n",
      "[    3   249  7951  2991  1177    19  1212     3    52   186  2164  3337\n",
      "     3 11648  1199    30  2339 24417   593     3]\n",
      "[ 0.  1.]\n",
      "\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[ 1.  0.]\n",
      "\n",
      "[    1  1582  1285  6594    97    12   198   107   585  1274  3118   495\n",
      "     4     1  7364  1472 13963     1  1286  2589]\n",
      "[ 0.  1.]\n",
      "\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[ 0.  1.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('X_train shape:', x_train.shape)\n",
    "print('X_test shape:', x_test.shape)\n",
    "print('X_val shape:', x_val.shape)\n",
    "\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "print('y_val shape:', y_val.shape)\n",
    "\n",
    "\n",
    "print(\"*****************************************\")\n",
    "print(\"Tokenized Reviews Train sets and their labels\")\n",
    "print(x_train[0][:20])\n",
    "print(y_train[0])\n",
    "print()\n",
    "print(x_train[1][:20])\n",
    "print(y_train[1])\n",
    "print()\n",
    "print(x_train[2][:20])\n",
    "print(y_train[2])\n",
    "print()\n",
    "print(x_train[3][:20])\n",
    "print(y_train[3])\n",
    "print()\n",
    "print(x_train[4][:20])\n",
    "print(y_train[4])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "********************************************\n",
    "\n",
    "As you can see the reviews have now been transformed into indices to tokenized vocabulary and the labels have been converted to one-hot encoding. We can now go ahead and feed these sequences to Neural Network Models.\n",
    "\n",
    "********************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A\n",
    "\n",
    "Building your first model (5 Points)\n",
    "\n",
    "Construct this sequential model using Keras :\n",
    "\n",
    "![title](img/model1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 250, 128)          3200000   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200)               6400200   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 402       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 9,600,602\n",
      "Trainable params: 9,600,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Built\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "## implement model here\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_len, 128, input_length=MAX_SEQUENCE_LENGTH ,trainable = True))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(200))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "## compille it here according to instructions\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "             optimizer = 'adam',\n",
    "             metrics = ['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "print(\"Model Built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/4\n",
      "20000/20000 [==============================] - 44s 2ms/step - loss: 0.4140 - acc: 0.7913 - val_loss: 0.3156 - val_acc: 0.8684\n",
      "Epoch 2/4\n",
      "20000/20000 [==============================] - 41s 2ms/step - loss: 0.0619 - acc: 0.9800 - val_loss: 0.4352 - val_acc: 0.8494\n",
      "Epoch 3/4\n",
      "20000/20000 [==============================] - 40s 2ms/step - loss: 0.0037 - acc: 0.9994 - val_loss: 0.5431 - val_acc: 0.8632\n",
      "Epoch 4/4\n",
      "20000/20000 [==============================] - 38s 2ms/step - loss: 2.5132e-04 - acc: 1.0000 - val_loss: 0.6152 - val_acc: 0.8624\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fab02107320>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=4,\n",
    "          validation_data=(x_val, y_val),\n",
    "          verbose = 1,\n",
    "         shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B\n",
    "\n",
    "Stacking Fully Connected Layers (5 points)\n",
    "\n",
    "Construct this sequential model using Keras :\n",
    "\n",
    "![title](img/model2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 250, 128)          3200000   \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 200)               6400200   \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 402       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 9,640,802\n",
      "Trainable params: 9,640,802\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Built\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "## implement model here\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_len, 128, input_length=MAX_SEQUENCE_LENGTH ,trainable = True))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(200))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(200))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "\n",
    "## compille it here according to instructions\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "             optimizer = 'adam',\n",
    "             metrics = ['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "print(\"Model Built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/4\n",
      "20000/20000 [==============================] - 35s 2ms/step - loss: 0.4021 - acc: 0.8008 - val_loss: 0.3089 - val_acc: 0.8706\n",
      "Epoch 2/4\n",
      "20000/20000 [==============================] - 35s 2ms/step - loss: 0.0486 - acc: 0.9842 - val_loss: 0.4224 - val_acc: 0.8484\n",
      "Epoch 3/4\n",
      "20000/20000 [==============================] - 35s 2ms/step - loss: 0.0058 - acc: 0.9983 - val_loss: 0.6826 - val_acc: 0.8500\n",
      "Epoch 4/4\n",
      "20000/20000 [==============================] - 35s 2ms/step - loss: 0.0135 - acc: 0.9956 - val_loss: 0.7160 - val_acc: 0.8430\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7faa99531780>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=4,\n",
    "          validation_data=(x_val, y_val),\n",
    "          verbose = 1,\n",
    "         shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part C\n",
    "\n",
    "Using LSTMS based networks(5 Points) \n",
    "\n",
    "Construct this sequential model using Keras :\n",
    "\n",
    "![title](img/model3.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 250, 128)          3200000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 3,348,354\n",
      "Trainable params: 3,348,354\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Built\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "## implement model here\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(vocab_len, 128, input_length=MAX_SEQUENCE_LENGTH ,trainable = True))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "## compille it here according to instructions\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "             optimizer = 'adam',\n",
    "             metrics = ['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print(\"Model Built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 234s 12ms/step - loss: 0.4383 - acc: 0.7915 - val_loss: 0.3428 - val_acc: 0.8594\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 227s 11ms/step - loss: 0.2059 - acc: 0.9224 - val_loss: 0.3708 - val_acc: 0.8612\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 229s 11ms/step - loss: 0.1093 - acc: 0.9608 - val_loss: 0.4354 - val_acc: 0.8590\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 233s 12ms/step - loss: 0.0718 - acc: 0.9757 - val_loss: 0.5474 - val_acc: 0.8618\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 236s 12ms/step - loss: 0.0433 - acc: 0.9860 - val_loss: 0.6439 - val_acc: 0.8614\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7faa991667f0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=5,\n",
    "          validation_data=(x_val, y_val),\n",
    "          verbose = 1,\n",
    "         shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part D\n",
    "\n",
    "Adding Pretrained Word Embeddings(10 Points)\n",
    "\n",
    "Construct this sequential model using Keras :\n",
    "\n",
    "Correction: The Embedding Layer Dimension (1st box) is 300, not 128.\n",
    "\n",
    "![title](img/model4.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 124252 unique tokens\n",
      "G Word embeddings: 1917494\n",
      "G Null word embeddings: 35772\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "\n",
    "#dimension of Glove Embeddings.\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "#load glove embeddings\n",
    "gembeddings_index = {}\n",
    "with codecs.open('glove.42B.300d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        gembedding = np.asarray(values[1:], dtype='float32')\n",
    "        gembeddings_index[word] = gembedding\n",
    "#\n",
    "f.close()\n",
    "print('G Word embeddings:', len(gembeddings_index))\n",
    "\n",
    "# nb_words contains the total length of vocab\n",
    "nb_words = len(word_index) +1\n",
    "\n",
    "#get glove embeddings for each word in tokenizer.\n",
    "#g_word_embedding_matrix holds the embeddings dictionary\n",
    "g_word_embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    gembedding_vector = gembeddings_index.get(word)\n",
    "    if gembedding_vector is not None:\n",
    "        g_word_embedding_matrix[i] = gembedding_vector\n",
    "        \n",
    "#total words in the tokenizer not in Embedding matrix\n",
    "print('G Null word embeddings: %d' % np.sum(np.sum(g_word_embedding_matrix, axis=1) == 0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 250, 300)          37275900  \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               219648    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 37,512,318\n",
      "Trainable params: 236,418\n",
      "Non-trainable params: 37,275,900\n",
      "_________________________________________________________________\n",
      "Model Built\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "## implement model here\n",
    "\n",
    "## to use the glove embeddings, your embedding layer would take the vocab size as input dimension, \n",
    "## Glove embedding dimension as the output dimsion\n",
    "## and you will provide the  embedding dictionary as the 'weights' parameter (!important) to the embedding layer.\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(Embedding(nb_words, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH,\n",
    "                    trainable = False,\n",
    "                    weights = [g_word_embedding_matrix]))\n",
    "model.add(LSTM(128,recurrent_dropout = 0.2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## compille it here according to instructions\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "             optimizer = 'adam',\n",
    "             metrics = ['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print(\"Model Built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 179s 9ms/step - loss: 0.4562 - acc: 0.7843 - val_loss: 0.3375 - val_acc: 0.8566\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 177s 9ms/step - loss: 0.2937 - acc: 0.8809 - val_loss: 0.3251 - val_acc: 0.8708\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 180s 9ms/step - loss: 0.2519 - acc: 0.8977 - val_loss: 0.3051 - val_acc: 0.8712\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 181s 9ms/step - loss: 0.2158 - acc: 0.9146 - val_loss: 0.2994 - val_acc: 0.8834\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 180s 9ms/step - loss: 0.1789 - acc: 0.9306 - val_loss: 0.3033 - val_acc: 0.8832\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa9f4d8e240>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=5,\n",
    "          validation_data=(x_val, y_val),\n",
    "          verbose = 1,\n",
    "         shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dont attempt this\n",
    "\n",
    "Stacking LSTM layers\n",
    "\n",
    "Unfortunately it takes very long to train, be aware we can stack LTMSs over each other like this.\n",
    "This requires bottom LSTM to return a sequences instead instead of single vector, which becomes input for the top LSTM.\n",
    "\n",
    "\n",
    "![title](img/model5.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part E\n",
    "\n",
    "Using Convolutional Networks (10 points)\n",
    "\n",
    "Construct the model, shown below. Use the same loss functions and optimizers as before\n",
    "\n",
    "Correction: The Embedding Layer Dimension (1st box) is 300, not 128.\n",
    "\n",
    "![title](img/model6.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 250, 300)          37275900  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 248, 128)          115328    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 248, 128)          0         \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 248, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 246, 64)           24640     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 246, 64)           0         \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 246, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 244, 32)           6176      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 244, 32)           0         \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 244, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 7808)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 256)               1999104   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2)                 514       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 39,421,662\n",
      "Trainable params: 2,145,762\n",
      "Non-trainable params: 37,275,900\n",
      "_________________________________________________________________\n",
      "Model Built\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "## implement model here\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(nb_words, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH,\n",
    "                    trainable = False,\n",
    "                    weights = [g_word_embedding_matrix]))\n",
    "\n",
    "model.add(Convolution1D(filters = 128, kernel_size = 3))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Convolution1D(filters = 64, kernel_size = 3))        \n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Convolution1D(filters = 32, kernel_size = 3))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## compille it here according to instructions\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "             optimizer = 'adam',\n",
    "             metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print(\"Model Built\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 50s 3ms/step - loss: 0.4481 - acc: 0.7749 - val_loss: 0.3648 - val_acc: 0.8398\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 48s 2ms/step - loss: 0.3005 - acc: 0.8758 - val_loss: 0.2903 - val_acc: 0.8784\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 48s 2ms/step - loss: 0.2607 - acc: 0.8934 - val_loss: 0.3458 - val_acc: 0.8568\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 49s 2ms/step - loss: 0.2292 - acc: 0.9082 - val_loss: 0.2967 - val_acc: 0.8814\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 48s 2ms/step - loss: 0.1972 - acc: 0.9207 - val_loss: 0.3159 - val_acc: 0.8862\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa9e265c940>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=5,\n",
    "          validation_data=(x_val, y_val),\n",
    "          verbose = 1,\n",
    "         shuffle = True)\n",
    "\n",
    "# Accuracy: 89.32%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part F\n",
    "\n",
    "Model constructed : (5 points)\n",
    "\n",
    "Test Accuracy Over 87.5%: (5 Points)\n",
    "\n",
    "Bonus: Min(10, Square of (test_score - 88%))\n",
    "\n",
    "Create your best model, use Validation score to judge your best model and check accuracy on test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 250, 300)          37275900  \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 128)               219648    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 37,512,318\n",
      "Trainable params: 236,418\n",
      "Non-trainable params: 37,275,900\n",
      "_________________________________________________________________\n",
      "Model Built\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "## implement model here\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(Embedding(nb_words, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH,\n",
    "                    trainable = False,\n",
    "                    weights = [g_word_embedding_matrix]))\n",
    "model.add(LSTM(128,recurrent_dropout = 0.2))\n",
    "model.add(Dense(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## compille it here according to instructions\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "             optimizer = 'adam',\n",
    "             metrics = ['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print(\"Model Built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model1: Question A   Accuracy: 86.76%    Run time OK\n",
    "# model2: Question B   Accuracy: 87.10%    Run time OK\n",
    "# model3: Question C   Accuracy: 84.89%    Run a long time\n",
    "# model4_D: Question D Accuracy: 89.62%\n",
    "# model5: Question E   Accuracy: 89.17%\n",
    "# model6: D modify dropout rate 0.3  Accuracy: 89.46%\n",
    "# model7 D modify dropout rate 0.1  recurrent_dropout = 0.1   89.53%\n",
    "# model8 D modify dropout rate 0.2  recurrent_dropout = 0.3   Accuracy: 89.61%\n",
    "# model 10 Add Activation Dense 89.24%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can keep saving models with different names in model_name, \n",
    "\n",
    "so you can retrieve their weights again for testing, you dont have to retrain \n",
    "(You would have to initialize the model definition again)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/7\n",
      "20000/20000 [==============================] - 225s 11ms/step - loss: 0.4478 - acc: 0.7907 - val_loss: 0.3697 - val_acc: 0.8420\n",
      "Epoch 2/7\n",
      "20000/20000 [==============================] - 216s 11ms/step - loss: 0.3003 - acc: 0.8750 - val_loss: 0.3060 - val_acc: 0.8702\n",
      "Epoch 3/7\n",
      "20000/20000 [==============================] - 207s 10ms/step - loss: 0.2489 - acc: 0.8992 - val_loss: 0.2937 - val_acc: 0.8804\n",
      "Epoch 4/7\n",
      "20000/20000 [==============================] - 205s 10ms/step - loss: 0.2139 - acc: 0.9144 - val_loss: 0.2925 - val_acc: 0.8808\n",
      "Epoch 5/7\n",
      "20000/20000 [==============================] - 209s 10ms/step - loss: 0.1688 - acc: 0.9347 - val_loss: 0.3040 - val_acc: 0.8862\n",
      "Epoch 6/7\n",
      "20000/20000 [==============================] - 220s 11ms/step - loss: 0.1286 - acc: 0.9511 - val_loss: 0.3751 - val_acc: 0.8720\n",
      "Epoch 7/7\n",
      "20000/20000 [==============================] - 207s 10ms/step - loss: 0.0861 - acc: 0.9681 - val_loss: 0.4536 - val_acc: 0.8858\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5cf2b85f98>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wt_dir = \"./weights/\"\n",
    "model_name = 'model10'\n",
    "early_stopping =EarlyStopping(monitor='val_acc', patience=2)\n",
    "bst_model_path = wt_dir + model_name + '.h5'\n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, monitor='val_acc', save_best_only=True, save_weights_only=True)\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=7,\n",
    "          validation_data=(x_val, y_val),\n",
    "          verbose = 1,\n",
    "         shuffle = True,\n",
    "         callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you plan on using Ensemble averaging, feel free to edit the code below or add multiple models.\n",
    "\n",
    "Make sure they get saved and can be retrieved when executing serially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_dir = \"./models/\"\n",
    "saved_model_path = model_dir + model_name + '.json'\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(saved_model_path, \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# load json and create model\n",
    "json_file = open('./models/model4_D.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "\n",
    "## compille it here according to instructions\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "             optimizer = 'adam',\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 82s 3ms/step\n",
      "Accuracy: 89.62%\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('./weights/model4_D.h5')\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part G\n",
    "\n",
    "Explain how Dense, LSTM and Convolution Layers work.\n",
    "\n",
    "Explain Relu, Dropout, and Softmax work.\n",
    "\n",
    "Analyze the architectures you constructed, with the accuracies you achieved and the training time it took. \n",
    "\n",
    "What are some insights you gained with these experiments? \n",
    "\n",
    "(5 Points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Dense Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dense layer is a fully connected layer. All the neurons in a layer are connected to those in the next layer. The Dense Layer represents a matrix vector multiplication. The values in the matrix are the trainable parameters which get updated during the backpropagation. The dense layer is used to change the dimensions of the vector. From the point of view of mathematic, the dense layer applies a rotation, scaling, translation transform to the vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM is short for Long Short Term Memeory Layer. The layer is applied for simple recurrent neural network which can be used as a building component or block of hidden layers for an finally bigger recurrent neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolution Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolution layer creates a convolution kernel that is convolved with the layer input over a single spatial dimension to produce a tensor of outputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relu function is f(x) = max(0,x). It is applied element-wise to the output of some other function. Relu improve neural networks's training speed. The gradient computation is simple and the computational step is easy. All the negative elements are set to 0. There is no exponentials, multiplication or division operations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using dropout can randomly deactivate certain neurons in a layer with a certain probability p from a Bernoulli distribution. The neural network won't be able to rely on particular activations in a given feed forward pass dring trainin. The neural network will learn different, redundant representations. What's more, the network can't reply on the particular neurons and the combination of these neurons to be present. The dropout layer doesn't have any trainable parameters.The dropout drops conections of neurons from the dense layer to prevent overfitting. The training will be fater when use the dropout layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A softmax layer is to convert any vector of real numbers into a vector of probabilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Architecture I constructed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Accuracy I achieved is 89.62%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The running time is 1489 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, add embedding layer, set the output dimension to be 300. Then add a LSTM layer, set the recurrent dropout to be 0.2. Set the dimension to be 128. Then set a dropout layer,set the rate to be 0.2. The training will be faster after adding this layer. Then add a dense layer, with the output dimension to be 128.Then add a Relu layer. Then add another dense layer and set the output dimension to be 2. In the end, add a softmax layer to convert vector of real numbers into a vector of probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the models are so large and it took some time to train the model that it is highly efficiency to use h5py to store models and load models. When design the architecture, change parameters and find the model that could have the highest accuracy when testing on test data, and also consider about the reducing running time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
